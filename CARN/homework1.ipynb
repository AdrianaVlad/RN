{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":115337,"databundleVersionId":13767164,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch.utils.data import Dataset\n\n\nimport pickle\nimport os\nimport pandas as pd\nimport numpy as np\n\n# You are not allowed to use DataLoader, torch.nn, torch.functional, torchvision\n# Backpropagation and data preprocessing must be implemented from scratch\n# Check https://github.com/Tensor-Reloaded/AI-Learning-Hub/blob/main/how_to/use_kaggle.md for learning how to use kaggle and submit to this competition\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:10:55.120824Z","iopub.execute_input":"2025-10-13T15:10:55.121063Z","iopub.status.idle":"2025-10-13T15:11:01.495759Z","shell.execute_reply.started":"2025-10-13T15:10:55.121040Z","shell.execute_reply":"2025-10-13T15:11:01.494947Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class ExtendedMNISTDataset(Dataset):\n    def __init__(self, root: str = \"/kaggle/input/fii-atnn-2025-competition-1\", train: bool = True):\n        file = \"extended_mnist_test.pkl\"\n        if train:\n            file = \"extended_mnist_train.pkl\"\n        file = os.path.join(root, file)\n        with open(file, \"rb\") as fp:\n            self.data = pickle.load(fp)\n\n    def __len__(self, ) -> int:\n        return len(self.data)\n\n    def __getitem__(self, i : int):\n        return self.data[i]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:11:01.496492Z","iopub.execute_input":"2025-10-13T15:11:01.496912Z","iopub.status.idle":"2025-10-13T15:11:01.502229Z","shell.execute_reply.started":"2025-10-13T15:11:01.496888Z","shell.execute_reply":"2025-10-13T15:11:01.501565Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_data = []\ntrain_labels = []\nfor image, label in ExtendedMNISTDataset(train=True):\n    train_data.append(image)\n    train_labels.append(label)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:11:01.503864Z","iopub.execute_input":"2025-10-13T15:11:01.504069Z","iopub.status.idle":"2025-10-13T15:11:02.086122Z","shell.execute_reply.started":"2025-10-13T15:11:01.504052Z","shell.execute_reply":"2025-10-13T15:11:02.085338Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"test_data = []\nfor image, label in ExtendedMNISTDataset(train=False):\n    test_data.append(image)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:11:02.086912Z","iopub.execute_input":"2025-10-13T15:11:02.087154Z","iopub.status.idle":"2025-10-13T15:11:02.178874Z","shell.execute_reply.started":"2025-10-13T15:11:02.087128Z","shell.execute_reply":"2025-10-13T15:11:02.178156Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport time\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nx_train = torch.tensor(train_data, dtype=torch.float32, device=device)\ny_train = torch.tensor(train_labels, dtype=torch.long, device=device)\nx_test  = torch.tensor(test_data, dtype=torch.float32, device=device)\n\nx_train = x_train.view(x_train.size(0), -1)\nx_test  = x_test.view(x_test.size(0), -1)\n\nx_train /= 255.0\nx_test  /= 255.0\n\n#torch.manual_seed(0)\n#x_val_list = []\n#y_val_list = []\n#x_train_list = []\n#y_train_list = []\n\n#for label in range(10):\n    #idx = (y_train == label).nonzero(as_tuple=True)[0]\n    #idx = idx[torch.randperm(len(idx))]\n    #split = int(0.1 * len(idx))\n    #x_val_list.append(x_train[idx[:split]])\n    #y_val_list.append(y_train[idx[:split]])\n    #x_train_list.append(x_train[idx[split:]])\n    #y_train_list.append(y_train[idx[split:]])\n\n#x_val = torch.cat(x_val_list, dim=0)\n#y_val = torch.cat(y_val_list, dim=0)\n#x_train = torch.cat(x_train_list, dim=0)\n#y_train = torch.cat(y_train_list, dim=0)\n\nval_split = int(0.1 * x_train.size(0))\nx_val, y_val = x_train[:val_split], y_train[:val_split]\nx_train, y_train = x_train[val_split:], y_train[val_split:]\n\ntorch.manual_seed(0)\nw1 = torch.randn(784, 100, device=device) * 0.01\nb1 = torch.zeros(1, 100, device=device)\nw2 = torch.randn(100, 10, device=device) * 0.01\nb2 = torch.zeros(1, 10, device=device)\n\nlr = 0.1\nepochs = 100\nbatch_size = 32\n\npatience = 10\nfactor = 0.5\nbest_val_loss = float('inf')\nno_improve_count = 0\n\nstart_time = time.time()\n\nfor epoch in range(epochs):\n    perm = torch.randperm(x_train.size(0))\n    x_train = x_train[perm]\n    y_train = y_train[perm]\n\n    total_loss = 0\n    correct = 0\n\n    for i in range(0, x_train.size(0), batch_size):\n        xb = x_train[i:i+batch_size]\n        yb = y_train[i:i+batch_size]\n\n        d1 = xb @ w1 + b1\n        a1 = torch.relu(d1)\n        d2 = a1 @ w2 + b2\n        loss = F.cross_entropy(d2, yb)\n        total_loss += loss.item() * len(xb)\n\n        probs = torch.softmax(d2, dim=1)\n        probs[range(len(yb)), yb] -= 1\n        probs /= len(yb)\n        dw2 = a1.T @ probs\n        db2 = probs.sum(0, keepdim=True)\n        da1 = probs @ w2.T\n        dd1 = da1.clone()\n        dd1[d1 <= 0] = 0\n        dw1 = xb.T @ dd1\n        db1 = dd1.sum(0, keepdim=True)\n\n        w1 -= lr * dw1\n        b1 -= lr * db1\n        w2 -= lr * dw2\n        b2 -= lr * db2\n\n        preds = torch.argmax(d2, dim=1)\n        correct += (preds == yb).sum().item()\n\n    train_acc = correct / x_train.size(0)\n    train_loss = total_loss / x_train.size(0)\n\n    #with torch.no_grad():\n    d1v = x_val @ w1 + b1\n    a1v = torch.relu(d1v)\n    d2v = a1v @ w2 + b2\n    val_loss = F.cross_entropy(d2v, y_val).item()\n    val_preds = torch.argmax(d2v, dim=1)\n    val_acc = (val_preds == y_val).float().mean().item()\n\n    if val_loss < best_val_loss - 1e-4:\n        best_val_loss = val_loss\n        no_improve_count = 0\n    else:\n        no_improve_count += 1\n        if no_improve_count >= patience:\n            lr *= factor\n            no_improve_count = 0\n            print(f\"   Learning rate reduced to {lr:.5f}\")\n\n    print(f\"Epoch {epoch+1:02d}: \"\n          f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, \"\n          f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}, lr={lr:.4f}\")\n\nprint(f\"Training done in {time.time() - start_time:.2f}s\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:11:02.179686Z","iopub.execute_input":"2025-10-13T15:11:02.179894Z","iopub.status.idle":"2025-10-13T15:13:03.674172Z","shell.execute_reply.started":"2025-10-13T15:11:02.179877Z","shell.execute_reply":"2025-10-13T15:13:03.673486Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/2305629170.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n  x_train = torch.tensor(train_data, dtype=torch.float32, device=device)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01: train_loss=0.4305, train_acc=0.8704, val_loss=0.2258, val_acc=0.9315, lr=0.1000\nEpoch 02: train_loss=0.1770, train_acc=0.9483, val_loss=0.1529, val_acc=0.9550, lr=0.1000\nEpoch 03: train_loss=0.1255, train_acc=0.9642, val_loss=0.1278, val_acc=0.9602, lr=0.1000\nEpoch 04: train_loss=0.0986, train_acc=0.9711, val_loss=0.1134, val_acc=0.9663, lr=0.1000\nEpoch 05: train_loss=0.0811, train_acc=0.9760, val_loss=0.1073, val_acc=0.9670, lr=0.1000\nEpoch 06: train_loss=0.0683, train_acc=0.9800, val_loss=0.0879, val_acc=0.9753, lr=0.1000\nEpoch 07: train_loss=0.0587, train_acc=0.9835, val_loss=0.0916, val_acc=0.9725, lr=0.1000\nEpoch 08: train_loss=0.0509, train_acc=0.9852, val_loss=0.0839, val_acc=0.9743, lr=0.1000\nEpoch 09: train_loss=0.0449, train_acc=0.9867, val_loss=0.0856, val_acc=0.9740, lr=0.1000\nEpoch 10: train_loss=0.0392, train_acc=0.9882, val_loss=0.0828, val_acc=0.9733, lr=0.1000\nEpoch 11: train_loss=0.0337, train_acc=0.9907, val_loss=0.0828, val_acc=0.9757, lr=0.1000\nEpoch 12: train_loss=0.0302, train_acc=0.9914, val_loss=0.0801, val_acc=0.9757, lr=0.1000\nEpoch 13: train_loss=0.0265, train_acc=0.9931, val_loss=0.0805, val_acc=0.9765, lr=0.1000\nEpoch 14: train_loss=0.0234, train_acc=0.9944, val_loss=0.0768, val_acc=0.9773, lr=0.1000\nEpoch 15: train_loss=0.0207, train_acc=0.9950, val_loss=0.0769, val_acc=0.9770, lr=0.1000\nEpoch 16: train_loss=0.0180, train_acc=0.9961, val_loss=0.0766, val_acc=0.9767, lr=0.1000\nEpoch 17: train_loss=0.0159, train_acc=0.9968, val_loss=0.0769, val_acc=0.9777, lr=0.1000\nEpoch 18: train_loss=0.0140, train_acc=0.9976, val_loss=0.0795, val_acc=0.9773, lr=0.1000\nEpoch 19: train_loss=0.0126, train_acc=0.9978, val_loss=0.0819, val_acc=0.9757, lr=0.1000\nEpoch 20: train_loss=0.0112, train_acc=0.9984, val_loss=0.0804, val_acc=0.9772, lr=0.1000\nEpoch 21: train_loss=0.0102, train_acc=0.9986, val_loss=0.0771, val_acc=0.9783, lr=0.1000\nEpoch 22: train_loss=0.0089, train_acc=0.9990, val_loss=0.0797, val_acc=0.9783, lr=0.1000\nEpoch 23: train_loss=0.0081, train_acc=0.9991, val_loss=0.0813, val_acc=0.9785, lr=0.1000\nEpoch 24: train_loss=0.0074, train_acc=0.9991, val_loss=0.0881, val_acc=0.9765, lr=0.1000\nEpoch 25: train_loss=0.0066, train_acc=0.9994, val_loss=0.0816, val_acc=0.9770, lr=0.1000\n   Learning rate reduced to 0.05000\nEpoch 26: train_loss=0.0060, train_acc=0.9995, val_loss=0.0837, val_acc=0.9775, lr=0.0500\nEpoch 27: train_loss=0.0047, train_acc=0.9998, val_loss=0.0815, val_acc=0.9773, lr=0.0500\nEpoch 28: train_loss=0.0045, train_acc=0.9998, val_loss=0.0818, val_acc=0.9778, lr=0.0500\nEpoch 29: train_loss=0.0043, train_acc=0.9998, val_loss=0.0821, val_acc=0.9772, lr=0.0500\nEpoch 30: train_loss=0.0041, train_acc=0.9998, val_loss=0.0827, val_acc=0.9775, lr=0.0500\nEpoch 31: train_loss=0.0040, train_acc=0.9999, val_loss=0.0827, val_acc=0.9773, lr=0.0500\nEpoch 32: train_loss=0.0038, train_acc=0.9999, val_loss=0.0829, val_acc=0.9773, lr=0.0500\nEpoch 33: train_loss=0.0037, train_acc=0.9999, val_loss=0.0839, val_acc=0.9780, lr=0.0500\nEpoch 34: train_loss=0.0036, train_acc=0.9999, val_loss=0.0828, val_acc=0.9777, lr=0.0500\nEpoch 35: train_loss=0.0035, train_acc=0.9999, val_loss=0.0828, val_acc=0.9777, lr=0.0500\n   Learning rate reduced to 0.02500\nEpoch 36: train_loss=0.0034, train_acc=0.9999, val_loss=0.0828, val_acc=0.9773, lr=0.0250\nEpoch 37: train_loss=0.0031, train_acc=0.9999, val_loss=0.0839, val_acc=0.9777, lr=0.0250\nEpoch 38: train_loss=0.0031, train_acc=1.0000, val_loss=0.0837, val_acc=0.9777, lr=0.0250\nEpoch 39: train_loss=0.0030, train_acc=0.9999, val_loss=0.0836, val_acc=0.9778, lr=0.0250\nEpoch 40: train_loss=0.0030, train_acc=1.0000, val_loss=0.0836, val_acc=0.9782, lr=0.0250\nEpoch 41: train_loss=0.0029, train_acc=0.9999, val_loss=0.0841, val_acc=0.9777, lr=0.0250\nEpoch 42: train_loss=0.0029, train_acc=1.0000, val_loss=0.0843, val_acc=0.9778, lr=0.0250\nEpoch 43: train_loss=0.0029, train_acc=1.0000, val_loss=0.0842, val_acc=0.9775, lr=0.0250\nEpoch 44: train_loss=0.0028, train_acc=1.0000, val_loss=0.0843, val_acc=0.9778, lr=0.0250\nEpoch 45: train_loss=0.0028, train_acc=1.0000, val_loss=0.0848, val_acc=0.9775, lr=0.0250\n   Learning rate reduced to 0.01250\nEpoch 46: train_loss=0.0027, train_acc=1.0000, val_loss=0.0846, val_acc=0.9780, lr=0.0125\nEpoch 47: train_loss=0.0027, train_acc=1.0000, val_loss=0.0846, val_acc=0.9778, lr=0.0125\nEpoch 48: train_loss=0.0027, train_acc=1.0000, val_loss=0.0846, val_acc=0.9778, lr=0.0125\nEpoch 49: train_loss=0.0026, train_acc=1.0000, val_loss=0.0847, val_acc=0.9777, lr=0.0125\nEpoch 50: train_loss=0.0026, train_acc=1.0000, val_loss=0.0849, val_acc=0.9778, lr=0.0125\nEpoch 51: train_loss=0.0026, train_acc=1.0000, val_loss=0.0849, val_acc=0.9775, lr=0.0125\nEpoch 52: train_loss=0.0026, train_acc=1.0000, val_loss=0.0852, val_acc=0.9777, lr=0.0125\nEpoch 53: train_loss=0.0026, train_acc=1.0000, val_loss=0.0846, val_acc=0.9777, lr=0.0125\nEpoch 54: train_loss=0.0026, train_acc=1.0000, val_loss=0.0848, val_acc=0.9778, lr=0.0125\nEpoch 55: train_loss=0.0025, train_acc=1.0000, val_loss=0.0849, val_acc=0.9777, lr=0.0125\n   Learning rate reduced to 0.00625\nEpoch 56: train_loss=0.0025, train_acc=1.0000, val_loss=0.0852, val_acc=0.9777, lr=0.0063\nEpoch 57: train_loss=0.0025, train_acc=1.0000, val_loss=0.0851, val_acc=0.9778, lr=0.0063\nEpoch 58: train_loss=0.0025, train_acc=1.0000, val_loss=0.0852, val_acc=0.9778, lr=0.0063\nEpoch 59: train_loss=0.0025, train_acc=1.0000, val_loss=0.0852, val_acc=0.9778, lr=0.0063\nEpoch 60: train_loss=0.0025, train_acc=1.0000, val_loss=0.0853, val_acc=0.9777, lr=0.0063\nEpoch 61: train_loss=0.0025, train_acc=1.0000, val_loss=0.0852, val_acc=0.9777, lr=0.0063\nEpoch 62: train_loss=0.0024, train_acc=1.0000, val_loss=0.0853, val_acc=0.9777, lr=0.0063\nEpoch 63: train_loss=0.0024, train_acc=1.0000, val_loss=0.0853, val_acc=0.9777, lr=0.0063\nEpoch 64: train_loss=0.0024, train_acc=1.0000, val_loss=0.0852, val_acc=0.9777, lr=0.0063\nEpoch 65: train_loss=0.0024, train_acc=1.0000, val_loss=0.0853, val_acc=0.9777, lr=0.0063\n   Learning rate reduced to 0.00313\nEpoch 66: train_loss=0.0024, train_acc=1.0000, val_loss=0.0854, val_acc=0.9777, lr=0.0031\nEpoch 67: train_loss=0.0024, train_acc=1.0000, val_loss=0.0853, val_acc=0.9777, lr=0.0031\nEpoch 68: train_loss=0.0024, train_acc=1.0000, val_loss=0.0854, val_acc=0.9777, lr=0.0031\nEpoch 69: train_loss=0.0024, train_acc=1.0000, val_loss=0.0854, val_acc=0.9777, lr=0.0031\nEpoch 70: train_loss=0.0024, train_acc=1.0000, val_loss=0.0854, val_acc=0.9777, lr=0.0031\nEpoch 71: train_loss=0.0024, train_acc=1.0000, val_loss=0.0854, val_acc=0.9777, lr=0.0031\nEpoch 72: train_loss=0.0024, train_acc=1.0000, val_loss=0.0854, val_acc=0.9777, lr=0.0031\nEpoch 73: train_loss=0.0024, train_acc=1.0000, val_loss=0.0855, val_acc=0.9777, lr=0.0031\nEpoch 74: train_loss=0.0024, train_acc=1.0000, val_loss=0.0855, val_acc=0.9777, lr=0.0031\nEpoch 75: train_loss=0.0024, train_acc=1.0000, val_loss=0.0855, val_acc=0.9777, lr=0.0031\n   Learning rate reduced to 0.00156\nEpoch 76: train_loss=0.0024, train_acc=1.0000, val_loss=0.0855, val_acc=0.9777, lr=0.0016\nEpoch 77: train_loss=0.0024, train_acc=1.0000, val_loss=0.0855, val_acc=0.9777, lr=0.0016\nEpoch 78: train_loss=0.0024, train_acc=1.0000, val_loss=0.0855, val_acc=0.9777, lr=0.0016\nEpoch 79: train_loss=0.0024, train_acc=1.0000, val_loss=0.0855, val_acc=0.9777, lr=0.0016\nEpoch 80: train_loss=0.0024, train_acc=1.0000, val_loss=0.0855, val_acc=0.9777, lr=0.0016\nEpoch 81: train_loss=0.0024, train_acc=1.0000, val_loss=0.0855, val_acc=0.9777, lr=0.0016\nEpoch 82: train_loss=0.0024, train_acc=1.0000, val_loss=0.0855, val_acc=0.9777, lr=0.0016\nEpoch 83: train_loss=0.0024, train_acc=1.0000, val_loss=0.0855, val_acc=0.9777, lr=0.0016\nEpoch 84: train_loss=0.0024, train_acc=1.0000, val_loss=0.0855, val_acc=0.9777, lr=0.0016\nEpoch 85: train_loss=0.0024, train_acc=1.0000, val_loss=0.0855, val_acc=0.9777, lr=0.0016\n   Learning rate reduced to 0.00078\nEpoch 86: train_loss=0.0023, train_acc=1.0000, val_loss=0.0855, val_acc=0.9777, lr=0.0008\nEpoch 87: train_loss=0.0023, train_acc=1.0000, val_loss=0.0855, val_acc=0.9777, lr=0.0008\nEpoch 88: train_loss=0.0023, train_acc=1.0000, val_loss=0.0855, val_acc=0.9777, lr=0.0008\nEpoch 89: train_loss=0.0023, train_acc=1.0000, val_loss=0.0855, val_acc=0.9777, lr=0.0008\nEpoch 90: train_loss=0.0023, train_acc=1.0000, val_loss=0.0856, val_acc=0.9777, lr=0.0008\nEpoch 91: train_loss=0.0023, train_acc=1.0000, val_loss=0.0856, val_acc=0.9777, lr=0.0008\nEpoch 92: train_loss=0.0023, train_acc=1.0000, val_loss=0.0856, val_acc=0.9777, lr=0.0008\nEpoch 93: train_loss=0.0023, train_acc=1.0000, val_loss=0.0856, val_acc=0.9777, lr=0.0008\nEpoch 94: train_loss=0.0023, train_acc=1.0000, val_loss=0.0856, val_acc=0.9777, lr=0.0008\nEpoch 95: train_loss=0.0023, train_acc=1.0000, val_loss=0.0856, val_acc=0.9777, lr=0.0008\n   Learning rate reduced to 0.00039\nEpoch 96: train_loss=0.0023, train_acc=1.0000, val_loss=0.0856, val_acc=0.9777, lr=0.0004\nEpoch 97: train_loss=0.0023, train_acc=1.0000, val_loss=0.0856, val_acc=0.9777, lr=0.0004\nEpoch 98: train_loss=0.0023, train_acc=1.0000, val_loss=0.0856, val_acc=0.9777, lr=0.0004\nEpoch 99: train_loss=0.0023, train_acc=1.0000, val_loss=0.0856, val_acc=0.9777, lr=0.0004\nEpoch 100: train_loss=0.0023, train_acc=1.0000, val_loss=0.0856, val_acc=0.9777, lr=0.0004\nTraining done in 113.11s\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\n\n#with torch.no_grad():\nd1_test = x_test @ w1 + b1\na1_test = torch.relu(d1_test)\nd2_test = a1_test @ w2 + b2\npredictions = torch.argmax(d2_test, dim=1).cpu().numpy()\n\npredictions_csv = {\n    \"ID\": list(range(len(predictions))),\n    \"target\": predictions.tolist(),\n}\n\ndf = pd.DataFrame(predictions_csv)\ndf.to_csv(\"submission.csv\", index=False)\nprint(\"Predictions saved to submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:13:03.674867Z","iopub.execute_input":"2025-10-13T15:13:03.675111Z","iopub.status.idle":"2025-10-13T15:13:03.701989Z","shell.execute_reply.started":"2025-10-13T15:13:03.675071Z","shell.execute_reply":"2025-10-13T15:13:03.701479Z"}},"outputs":[{"name":"stdout","text":"Predictions saved to submission.csv\n","output_type":"stream"}],"execution_count":6}]}